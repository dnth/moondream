{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f474a869642e409691a6282d7657b2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:cuda\n",
      "Type: torch.float16\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from moondream_model import VisionEncoder, TextModel\n",
    "import torch \n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "model_path = snapshot_download(\"vikhyatk/moondream1\")\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "DTYPE = torch.float16\n",
    "\n",
    "vision_encoder = VisionEncoder(model_path).to(DEVICE, dtype=DTYPE)\n",
    "text_model = TextModel(model_path).to(DEVICE, dtype=DTYPE)\n",
    "\n",
    "print(f\"Using:{DEVICE}\")\n",
    "print(f\"Type: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_inference(image, prompt, max_new_tokens=128):\n",
    "#     with torch.inference_mode(), torch.cuda.amp.autocast():\n",
    "#         image_embeds = vision_encoder(image)\n",
    "#         result = text_model.answer_question(image_embeds, \n",
    "#                                             prompt, \n",
    "#                                             max_new_tokens=max_new_tokens)\n",
    "    \n",
    "#     if isinstance(result, tuple):\n",
    "#         result_text = result[0]\n",
    "#     else:\n",
    "#         result_text = result\n",
    "\n",
    "#     # Convert the result to string if it's not already\n",
    "#     if not isinstance(result_text, str):\n",
    "#         if torch.is_tensor(result_text):\n",
    "#             result_text = result_text.cpu().numpy().tolist()\n",
    "#             result_text = ' '.join(map(str, result_text))\n",
    "#         else:\n",
    "#             result_text = str(result_text)\n",
    "\n",
    "#     # Apply regex to clean up the result string\n",
    "#     cleaned_result = re.sub(\"<$\", \"\", re.sub(\"END$\", \"\", result_text))\n",
    "#     return cleaned_result\n",
    "\n",
    "# Pre-compiled regular expressions (outside the function)\n",
    "end_pattern = re.compile(r\"END$\")\n",
    "lt_pattern = re.compile(r\"<$\")\n",
    "\n",
    "def run_inference(image, prompt, max_new_tokens=128):\n",
    "    with torch.inference_mode():\n",
    "        image_embeds = vision_encoder(image)\n",
    "        result = text_model.answer_question(image_embeds, prompt, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    if isinstance(result, tuple):\n",
    "        result_text = result[0]\n",
    "    else:\n",
    "        result_text = result\n",
    "\n",
    "    # Directly converting tensor to string if it's not a string\n",
    "    if not isinstance(result_text, str):\n",
    "        if torch.is_tensor(result_text):\n",
    "            result_text = ' '.join(map(str, result_text.tolist()))\n",
    "        else:\n",
    "            result_text = str(result_text)\n",
    "\n",
    "    # Optimized regex application\n",
    "    cleaned_result = lt_pattern.sub(\"\", end_pattern.sub(\"\", result_text))\n",
    "    return cleaned_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"img/output_000017.jpg\")\n",
    "prompt = \"Describe this image.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time took 0.47188782691955566 seconds.\n",
      "Inference time took 0.46398043632507324 seconds.\n",
      "Inference time took 0.45852208137512207 seconds.\n",
      "Inference time took 0.4499180316925049 seconds.\n",
      "Inference time took 0.44845128059387207 seconds.\n",
      "Inference time took 0.4537317752838135 seconds.\n",
      "Inference time took 0.4492359161376953 seconds.\n",
      "Inference time took 0.44809603691101074 seconds.\n",
      "460 ms ± 5.78 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result = run_inference(img, prompt, max_new_tokens=20)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moondream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
